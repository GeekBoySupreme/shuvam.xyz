<!-- Start header block from here -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
<title>Building Brunch</title>
<link rel="stylesheet" href="../article-style.css"/>
<link rel="icon" href="../../favicon.ico">

<script src="https://kit.fontawesome.com/622afd0c30.js" crossorigin="anonymous"></script>
</head>

<body>
<!-- Copy till here -->

<article id="1fdd39d6-c19a-8036-9023-f5dcdeaa4d11" class="page sans"><header><div class="page-header-icon undefined"><img class="icon" src="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/brunch-logo.png"/></div><h1 class="page-title">Building Brunch</h1><p class="page-description"></p></header><div class="page-body"><p id="1fdd39d6-c19a-80c9-8b14-d48588aa62bb" class="">When GPT-3 came out in late 2020, I started exploring a bunch of prototypes on how we can build adaptive user interfaces. The idea being that how we generate interfaces can (at least for a brief period in history) afford to be steered by language models that do not need strict scaffolding to generate bits of personalized interfaces. </p><p id="1fdd39d6-c19a-804e-bb5f-e81f20723af5" class="">
</p><p id="1fdd39d6-c19a-805d-bbb0-eb51b1482b4d" class="">After ChatGPT came out, technology circles erupted with the debate on whether a <code>chat</code> is the best interface to become the anvil of shaping the thoughts of an LLM. Part of the discourse was because chat is sort of a power steering for structuring indeterministic outs, and part was whether is a correct representation on how we explore our thoughts. As humans, we don‚Äôt think in a linear fashion, we go in different directions and then we go off in some more branches until probably some of the branches interwine and converge back with the main root, creating the seemingly perfect thought that makes things condense to a circle.</p><p id="1fdd39d6-c19a-80d2-9017-f4ce123aa5f7" class="">While working with multiple LLMs and exploring and structuring ideas, it became cumbersome to switch between multiple models and multiple tabs, copy pasting ideas from one tab to the other and passing context between multiple tabs became frustrating. Hence, I started building <code>Brunch</code>‚éØ a graphical node-based interface to watch exchanges between different instances of GPT-3, each one initialized with a different prompt to encourage divergence of outputs. For instance, one API call has a system prompt to make it think more along the good/moral side of things : the proverbial angel to steer your thoughts and ideas in generally a good direction, while another has a system prompt set to make it act like the devil. </p><p id="1fdd39d6-c19a-80b7-9b8b-f44730175f20" class="">This experiment evolved to then use multiple language models along with a few more interesting ideas baked in along the way.</p><p id="1fdd39d6-c19a-8073-aa6d-d1bfe20d5fe7" class="">
</p><figure id="1fdd39d6-c19a-8052-95f4-d0b5e7cd7046" class="image"><a target="_blank" href="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image.png"><img style="width:3024px" src="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image.png"/></a></figure><p id="1fdd39d6-c19a-80c0-9f8d-ea7f7d7896a0" class="">In 2025, a lot of these ideas/optimizations are no longer novel, but exploring these ideas was fun a couple of years back, and this essay deals with explaining some of them, and how they evolved over these years.</p><p id="1fed39d6-c19a-8023-991b-d8581e58d586" class="">
</p><h3 id="1fed39d6-c19a-802f-8293-e9fec70fafa3" class="">Freedom to select a model</h3><hr id="1fed39d6-c19a-808c-92c4-ed93da9f2cf1"/><figure id="1fed39d6-c19a-80b8-a9c9-eb4e1413d23a" class="image"><a target="_blank" href="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%201.png"><img style="width:3024px" src="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%201.png"/></a></figure><p id="1fdd39d6-c19a-8060-bcd2-df7263ae759c" class="">When I started experimenting with LLMs and a deluge of them came out all at once, it became imperative for me that regardless of what tool or experiment I build, it needs to come with the freedom of selecting and using the models I want, mixing and inter-mingling them seamlessly without having to constantly switch tabs. And while these affordances to activate a model eases things significantly, I added another way to actually use them while you are working with LLMs.</p><p id="1fed39d6-c19a-8004-8f1d-f277c021226b" class="">Which brings us to‚Ä¶</p><p id="1fdd39d6-c19a-8019-a79e-c3719bc355b0" class="">
</p><h3 id="1fdd39d6-c19a-80e4-8344-fc38b85f6c76" class="">Implicit Invocation</h3><hr id="1fdd39d6-c19a-8070-94a7-c8bbc357cd68"/><p id="1fdd39d6-c19a-80e8-a76a-d04288adb141" class="">What if you could invoke any model (on in an earlier case : different instances of GPT-3 itself linked with different aliases) by simply adding the name string in the prompt. So if my instance is aliased ‚ÄúCoffee‚Äù, I can simply add the text, ‚ÄúHey Coffee, I want you to‚Ä¶‚Äù and it would specifically be ‚ÄúCoffee‚Äù responding to my prompt.</p><p id="1fdd39d6-c19a-8093-a847-e4b63abc2339" class="">It was earlier simply scanning the input string to find the relevant substring and then routing the API call to that relevant instance. </p><p id="1fdd39d6-c19a-80b0-a297-d91d9f52baef" class="">While rewriting the older codebase for Brunch, I added a few optimizations on top of the basic string detection. </p><ul id="1fdd39d6-c19a-8006-b6cd-f159b9cbfabf" class="bulleted-list"><li style="list-style-type:disc">I added another instance of GPT 3.5 tuned to be a classifier, I pass a system prompt that outlines what each of the model is strong at, and then automatically returns a single string from the set of language models that had been added to the system prompt. If if the instance ‚Äúfeels‚Äù that Claude is the best model to answer the question, it should simply output <code>claude</code>, or if Gemini can handle the question well, it should simply give me <code>gemini</code>. </li></ul><ul id="1fdd39d6-c19a-804a-a427-d22c45fbd8c2" class="bulleted-list"><li style="list-style-type:disc">And now, based on the output string, we use a simple switch case to route the input prompt to the suitable LLM API.</li></ul><p id="1fdd39d6-c19a-8082-b990-f5cc1459b477" class="">
</p><h3 id="1fdd39d6-c19a-808d-86f3-fb553514696d" class="">Smart Clustering</h3><hr id="1fdd39d6-c19a-804d-ae6b-cd97b281ebd5"/><p id="1fdd39d6-c19a-807b-bbeb-cb7cb703ef4a" class="">This behaviour is still slightly flaky, but one instance of GPT 3.5 keeps scanning all the cards/thoughts/extensions generated and arranges them into contextual smart clusters, naming the cluster with a context-relevant string.</p><figure id="1fdd39d6-c19a-80df-b6b8-e593cdc83e8e" class="image"><a target="_blank" href="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%202.png"><img style="width:3024px" src="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%202.png"/></a></figure><p id="1fdd39d6-c19a-8048-a961-c0907c51ed03" class="">
</p><h3 id="1fdd39d6-c19a-80f5-b834-e9d8bf099b01" class="">Add Context</h3><hr id="1fdd39d6-c19a-800b-92a1-cbd44a028d80"/><p id="1fdd39d6-c19a-8003-a631-f73b8f89c08c" class="">Before expanding a thought or creating more adjacent thoughts, you can now add a separate custom string to steer the conversation in any direction that you like. </p><p id="1fdd39d6-c19a-80bc-83d2-c7095ded1297" class="">You can add additional prompts and have more granular control on how you can steer the conversation.</p><p id="1fdd39d6-c19a-8016-a746-d0e5a17b6e1c" class="">
</p><h3 id="1fdd39d6-c19a-806c-8c60-e7eabff52ccd" class="">Chain of Thought</h3><hr id="1fdd39d6-c19a-8053-b10f-f426cb742953"/><p id="1fdd39d6-c19a-8057-869e-e5b5d64a6ab6" class="">When you are exploring the outputs of multiple LLMs across a bunch of diverse and scattered thoughts, it becomes important to have an ability to trace how the thought evolved. This bit was really fun as it required some pretty gnarly javascript wrangling to ensure it works as intended but when you click on a specific thought/card, it triggers a recursive lookup for parent cards until the traversal reaches the very root card which obviously wouldn‚Äôt have any more parent attached to it.</p><figure id="1fdd39d6-c19a-80c6-8d90-e282217b6c62" class="image"><a target="_blank" href="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%203.png"><img style="width:3024px" src="Building%20Brunch%201fdd39d6c19a80369023f5dcdeaa4d11/image%203.png"/></a></figure><p id="1fdd39d6-c19a-8059-a919-c38a7b022370" class="">Brunch was entirely written in vanilla <code>HTML</code>, <code>CSS</code>, and <code>Javascript</code>. Significant chunks of the chain of thought traversal logic was courtesy Claude (and honestly, 3.7 Sonnet is the best model to give you comparatively cleaner, nicely modularized code). </p><p id="1fdd39d6-c19a-80b1-934f-fe5ec4c1ff7f" class="">It was fun to add voice input and smaller fun bits like <code>The machines are thinking...</code>.</p><p id="1fdd39d6-c19a-8082-90fb-c195e7d7d620" class="">You can take <code>Brunch</code> for a spin at ‚éØ <code><a target="_blank" href="https://brunch.shuvam.xyz">brunch.shuvam.xyz</a></code> </p><p id="1fdd39d6-c19a-807f-9417-ed1c25b3f0b2" class="">Feel free to send your thoughts over on <code><a target="_blank" href="https://bsky.app/profile/shuvam.bsky.social">Bluesky</a></code> or <code><a target="_blank" href="https://x.com/shuvam360">Twitter</a></code> üññ</p><p id="1fdd39d6-c19a-80b1-a011-f5aed8dd767b" class="">
</p><p id="1fed39d6-c19a-80f3-805e-c76b4d08755a" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>
